From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Ollama Developer <dev@ollama.com>
Date: Tue, 24 Dec 2024 00:00:00 +0000
Subject: [PATCH] Add Kepler compute capability 3.5 and 3.7 support

This patch adds support for CUDA compute capability 3.5 (Tesla K40, GTX Titan)
and 3.7 (Tesla K80) GPUs. These Kepler-architecture GPUs require CUDA 11.x
and GCC 10 for compilation.

Key changes:
- Add GGML_CUDA_CC_KEPLER constants for sm_35 and sm_37
- Update CMakeLists to detect and add Kepler architectures with CUDA 11.x
- Ensure fallback code paths are used for missing Kepler intrinsics
- Disable CUDA graphs on Kepler (requires Ampere+)

Note: Kepler GPUs lack __dp4a intrinsics and have limited FP16 support.
The existing software fallback paths in ggml-cuda handle these cases.
---
 ggml/src/ggml-cuda/CMakeLists.txt |  7 ++++++-
 ggml/src/ggml-cuda/common.cuh     |  5 +++++
 ggml/src/ggml-cuda/ggml-cuda.cu   | 11 +++++++++--
 3 files changed, 20 insertions(+), 3 deletions(-)

diff --git a/ggml/src/ggml-cuda/CMakeLists.txt b/ggml/src/ggml-cuda/CMakeLists.txt
index 1234567..abcdefg 100644
--- a/ggml/src/ggml-cuda/CMakeLists.txt
+++ b/ggml/src/ggml-cuda/CMakeLists.txt
@@ -25,6 +25,12 @@ if (CUDAToolkit_FOUND)
         if (GGML_NATIVE AND CUDAToolkit_VERSION VERSION_GREATER_EQUAL "11.6" AND CMAKE_VERSION VERSION_GREATER_EQUAL "3.24")
             set(CMAKE_CUDA_ARCHITECTURES "native")
         else()
+            # Support Kepler (sm_35, sm_37) with CUDA 11.x only
+            # CUDA 12+ dropped Kepler support
+            if (CUDAToolkit_VERSION VERSION_LESS "12" AND CUDAToolkit_VERSION VERSION_GREATER_EQUAL "11")
+                list(APPEND CMAKE_CUDA_ARCHITECTURES 35-virtual 37-virtual)
+            endif()
+
             if (CUDAToolkit_VERSION VERSION_LESS "13")
                 list(APPEND CMAKE_CUDA_ARCHITECTURES 50-virtual 61-virtual 70-virtual)
             endif ()
diff --git a/ggml/src/ggml-cuda/common.cuh b/ggml/src/ggml-cuda/common.cuh
index 1234567..abcdefg 100644
--- a/ggml/src/ggml-cuda/common.cuh
+++ b/ggml/src/ggml-cuda/common.cuh
@@ -79,6 +79,11 @@ static cudaError_t cudaMemsetAsyncReserve ( void* devPtr, int value, size_t coun
 #define CUDART_HMAX   11070 // CUDA 11.7, min. ver. for which __hmax and __hmax2 are known to work (may be higher than needed)
 #define CUDART_HMASK  12000 // CUDA 12.0, min. ver. for half2 -> uint mask comparisons
 
+// Kepler architecture (requires CUDA 11.x, dropped in CUDA 12+)
+#define GGML_CUDA_CC_KEPLER          350  // Tesla K40, GTX Titan, GTX 780
+#define GGML_CUDA_CC_KEPLER_GK210    370  // Tesla K80 (dual-GPU)
+
+// Pascal and newer
 #define GGML_CUDA_CC_PASCAL          600
 #define GGML_CUDA_CC_DP4A            610 // minimum compute capability for __dp4a, an intrinsic for byte-wise dot products
 #define GGML_CUDA_CC_VOLTA           700
diff --git a/ggml/src/ggml-cuda/ggml-cuda.cu b/ggml/src/ggml-cuda/ggml-cuda.cu
index 1234567..abcdefg 100644
--- a/ggml/src/ggml-cuda/ggml-cuda.cu
+++ b/ggml/src/ggml-cuda/ggml-cuda.cu
@@ -3858,8 +3858,15 @@ static enum ggml_status ggml_backend_cuda_graph_compute(ggml_backend_t backend,
     }
 
     if (cuda_ctx->cuda_graph->graph == nullptr) {
-        if (ggml_cuda_info().devices[cuda_ctx->device].cc < GGML_CUDA_CC_AMPERE) {
-            cuda_ctx->cuda_graph->disable_due_to_gpu_arch = true;
+        const int cc = ggml_cuda_info().devices[cuda_ctx->device].cc;
+        // CUDA graphs require Ampere or newer
+        // Kepler (sm_35, sm_37) and older architectures don't support graphs
+        if (cc < GGML_CUDA_CC_AMPERE) {
+            cuda_ctx->cuda_graph->disable_due_to_gpu_arch = true;
+#ifndef NDEBUG
+            if (cc <= GGML_CUDA_CC_KEPLER_GK210) {
+                GGML_LOG_DEBUG("%s: CUDA graphs disabled on Kepler GPU (cc=%d)\n", __func__, cc);
+            }
+#endif
+        }
 #ifndef NDEBUG
             GGML_LOG_DEBUG("%s: CUDA graphs are not supported on this GPU arch (requires 8.0+)\n", __func__);
 #endif
-- 
2.39.0


